{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f58e1f",
   "metadata": {},
   "source": [
    "# Binary classification on a HEP dataset\n",
    "\n",
    "In the tutorial we'll set aside the fundamentals and focus on the practical stuff: how to train a deep neural network on a HEP dataset.\n",
    "Our problem will still be binary classification: distinguishing between signal and background.\n",
    "This is a simple and very common problem in HEP, which can also be surprisingly subtle.\n",
    "Entire working groups within experimental collaborations are dedicated to just this task.\n",
    "\n",
    "The dataset is from collider physics, but the methods and typical problems you might encounter should carry over to data from other HEP contexts, e.g. neutrino experiments.\n",
    "\n",
    "We'll use pytorch again. If you plan to use tensorflow or JAX for your own work, translate the notebook as an exercise!\n",
    "\n",
    "Outline:\n",
    "- Load data from root files\n",
    "- Explore the data and weights\n",
    "- Preprocess data for training\n",
    "- How to use GPU resources\n",
    "- Train / test / val splits\n",
    "- Train a neural network\n",
    "- Quantify its performance\n",
    "- Tune the model and overtraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62fb293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff31d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use GPU acceleration in this tutorial. Let's check if we have one available.\n",
    "print(f\"Is CUDA available? {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs available: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1398a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change the \"0\" here to 1, 2, or 3 to use other GPUs on the node for the rest of the notebook\n",
    "dnumber = 0\n",
    "device = torch.device(f\"cuda:{dnumber}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "device_name = torch.cuda.get_device_name(dnumber)\n",
    "print(f\"Device name: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa5153b",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "This data set was produced from the ATLAS open data 2020 release:\n",
    "\n",
    "https://opendata.atlas.cern/docs/category/13-tev-2020-release\n",
    "\n",
    "The dataset contains MC simulations of the following processes that occur within LHC collisions:\n",
    "\n",
    "- Higgs boson production, where the Higgs decays to a pair of W boson that then decay leptonically: $H \\rightarrow WW \\rightarrow l \\nu l \\nu$. We want to isolate this signal so we can study this Higgs boson.\n",
    "- Other standard model processes: top-quark-pair production, single-top production, electroweak boson production in association with jets (W+jets, Z+jets), and diboson production (WW, WZ, ZZ). Jets are a columnated spray of hadrons that are the experimental signature of a quark or gluon produced with large transverse momentum. All of these processes can also result in final states with two leptons and significant missing transverse momentum, making them backgrounds. If we define some signal region that should contain $H \\rightarrow WW \\rightarrow l \\nu l \\nu$ events, events some from of these background processes will contaminate it and degrade the sensitivity of our result. So, we want to remove these backgrounds.\n",
    "\n",
    "The binary classification task will be to discriminate signal ($H \\rightarrow WW$) from background (all of the other SM processes).\n",
    "Our data will be a list of kinematic features that characterize each event.\n",
    "These features should be somewhat different between the signal and background in non-trivial ways, and our classifier will use these differences to separate the two classes.\n",
    "\n",
    "All the data are stored in a root file, which contains different kinematic variables of each event, the event label (`label`) to indicate whether an event is a signal or background, as well as the MC event weights (`mcWeight`). We use [uproot](https://uproot.readthedocs.io/en/latest/) to read data from the root file.\n",
    "Given the prevalence of root files in HEP, and the prevalence of Python in machine learning, uproot is an indispensible tool for applications of ML in HEP.\n",
    "Learn to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataWW_d1.root\"\n",
    "file = uproot.open(filename)\n",
    "\n",
    "# show what is inside the root file loaded from uproot\n",
    "print(file.classnames())\n",
    "\n",
    "tree = file[\"tree_event\"]  # select the TTree inside the root file\n",
    "tree.show()  # show all the branches inside the TTree\n",
    "dfall = tree.arrays(library=\"pd\")  # convert uproot TTree into pandas dataframe\n",
    "print(\"============================================\")\n",
    "print(\"File loaded with \", len(dfall), \" events \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump list of feature\n",
    "dfall.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26438441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine first few events\n",
    "dfall.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16acf98",
   "metadata": {},
   "source": [
    "We have 800k events in this data file, which sits in a nice medium range where we have plenty of data for training but not so much data that we will run into memory constraints.\n",
    "What to do in cases when this is not true is beyond the scope for today, but keep in mind it didn't need to be this way, and often won't be in real world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df4607d",
   "metadata": {},
   "source": [
    "## Event selection\n",
    "\n",
    "In this example, we want to focus on events with exactly 2 leptons (electrons or muons).\n",
    "There are both signal and background events where this will not be the case, e.g. if a lepton is missed by the detector, but this is a small effect that we can ignore for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faaa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of events before selections:\", len(dfall))\n",
    "fulldata = dfall[dfall.lep_n == 2]\n",
    "print(\"Number of events after selections:\", len(fulldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fbdce",
   "metadata": {},
   "source": [
    "In collider physics, a common feature of simulated datasets is event weights.\n",
    "These weights arise from the beyond leading order Feynman diagram calculations that occur within event generators.\n",
    "Without the weights, the simulation will not resemble real data with as much accuracy.\n",
    "If you're a collider physicist, you're probably already familiar with these weights.\n",
    "If not, you can ignore this detail and just continue on in the notebook.\n",
    "As you'll see, what we'll have to do to handle these weights is actually quite minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b5e132",
   "metadata": {},
   "source": [
    "Let's load the weights, stored in the column `mcWeight` and the labels, stored in the column `target`. Like in this morning's tutorial, the labels will take on a value of 1.0 for signal and 0.0 for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef38dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(fulldata.label)\n",
    "weights = np.array(fulldata.mcWeight)\n",
    "\n",
    "print(\"Number of selected signal events:\", len(fulldata[target == 1]))\n",
    "print(\"Number of selected background events:\", len(fulldata[target == 0]))\n",
    "\n",
    "# plot the distribution of the weights\n",
    "plt.hist(weights, bins=100, log=True)\n",
    "plt.xlabel(\"Event weight\")\n",
    "plt.ylabel(\"Number of events\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean weight: {weights.mean()}\")\n",
    "print(f\"Std weight: {weights.std()}\")\n",
    "print(f\"Max weight: {weights.max()}\")\n",
    "print(f\"Min weight: {weights.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b60eb4c",
   "metadata": {},
   "source": [
    "For the collider physicists: a few things to note about this weight distribution. First, the mean weight is very small, only about 0.0002. This reduces the effective number of events in our data sample, but as we'll see it doesn't actually matter for training the network. Second, some of these weights are negative. This may seem like a problem, and many data science tools will break if asked to work with some negative event weights, but in reality we can train networks with a small fraction of negative weights with no issue, as we'll do so here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724eea42",
   "metadata": {},
   "source": [
    "Next we need to decide which features we'll use to disciminate the signal from background. Tho start we'll just use 6 features, but one of the great things about machine learning based classifiers, like neural networks or BDTs, is they can easily accommodate and will benefit from more features, assuming those features are pre-processed properly. These extra features can be either orthogonal features that tell the network something new about the events, or \"engineered\" features which we know, perhaps from thinking about the underlying physics, are useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db2a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(fulldata, columns=[\"met_et\", \"met_phi\", \"lep_pt_0\", \"lep_pt_1\", 'lep_eta_0', 'lep_eta_1'])\n",
    "\n",
    "# Or we can use more features to improve the discrimination power:\n",
    "# data=pd.DataFrame(fulldata, columns=[\"met_et\",\"met_phi\",\"lep_pt_0\",\"lep_pt_1\",'lep_eta_0', 'lep_eta_1',\n",
    "#                                      'lep_phi_0', 'lep_phi_1','jet_n','jet_pt_0',\n",
    "#                                      'jet_pt_1', 'jet_eta_0', 'jet_eta_1', 'jet_phi_0', 'jet_phi_1'])\n",
    "\n",
    "# We can also engineer our own feature:\n",
    "# data[\"lep_deltaphi\"]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d61196",
   "metadata": {},
   "source": [
    "What are these features?\n",
    "\n",
    "- `met_et` is the missing transverse momentum in the event in units of GeV. We expect to see a large amount of missing transverse momentum in the signal class, because of the two neutrinos in the final state.\n",
    "- `lep_pt_0` and `lep_pt_1` are the transverse momentum of the leading (higher $p_T$) and sub-leading (lower $p_T$) leptons in units of GeV.\n",
    "- `met_phi` is the azimuthal angle of the missing transverse momentum vector\n",
    "- `lep_eta_0` and `lep_eta_1` are the pseudorapidity of the leading and sub-leading leptons, respectively\n",
    "\n",
    "If you're unfamiliar with these coordinates, see the Figure below (pseudorapidity is denoted $\\eta$).\n",
    "Note the azimuthal angle observable offers no separation between signal and background, as we could expect from symmetry arguments.\n",
    "You can get rid of it from what follows and expect no differences in performance, but we can also include it and see no drop in performance.\n",
    "In general, neural networks and BDT based binary classifiers are very good at ignoring irrelevant features in most applications.\n",
    "There's generally no risk to adding extra features as input.\n",
    "If you're tring to maximize performance, the best advice is to use everything!\n",
    "\n",
    "<img src=\"axis3D_CMS-005.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ec6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = data[target == 0].hist(\n",
    "    weights=weights[target == 0], figsize=(15, 12), color='b', alpha=0.4, density=True, label=\"signal\", bins=100\n",
    ")\n",
    "ax = ax.flatten()[: data.shape[1]]  # to avoid error if holes in the grid of plots (like if 7 or 8 features)\n",
    "data[target == 1].hist(\n",
    "    weights=weights[target == 1], figsize=(15, 12), color='r', alpha=0.4, density=True, ax=ax, label=\"background\", bins=100\n",
    ")\n",
    "for i in range(len(ax)):\n",
    "    ax[i].legend(loc='right')\n",
    "    if not \"phi\" in data.keys()[i]:\n",
    "        ax[i].set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b191135",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "The scale of the inputs we will use as input to our network vary significantly.\n",
    "For example the dimensionful inputs can have values up to several thousand, but the azimuthal angle and pseudorapidity inputs are bounded and symmetrically distributed about zero.\n",
    "In network training, large scale inputs can have significant negative impacts, making the optimization either very difficult, or causing it to break entirely.\n",
    "It's always best to make histograms of your inputs prior to training, and ensure that they have some \"reasonable\" distribution..\n",
    "\n",
    "These are the 2 most common ways to rescale a given input:\n",
    "\n",
    "1. **Scale to Mean of 0 and Variance of 1.0:**   $\\ \\ \\ \\ x^\\prime = (x-\\mu)/\\sigma$\n",
    "2. **Scale to Max of 1 and Min of 0:**   $\\ \\ \\ \\ x^\\prime = (x-x_{\\mathrm{min}})/(x_{\\mathrm{max}}-x_{\\mathrm{min}})$\n",
    "\n",
    "In the following, we'll take the first approach. One way to do this would be to apply the scaling right now, to all of the data, and forget about it for the rest of the notebook.\n",
    "However I personally dislike this approach because if you ever want to use the trained network later, you'll need to remember exactly what scaling you applied to the data.\n",
    "This is a great way to introduce a hard to find bug.\n",
    "Instead, I find it useful to make the rescaling a part of the network itself.\n",
    "We'll do this below.\n",
    "All we need to do for now is calculate the mean and std. deviation of our input observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array(data.mean(axis=0))\n",
    "stds = np.array(data.std(axis=0))\n",
    "\n",
    "print(\"Means:\")\n",
    "print(means)\n",
    "print(\"Stds:\")\n",
    "print(stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0a591d",
   "metadata": {},
   "source": [
    "## Adjust the Signal/Background Weights for balanced training\n",
    "\n",
    "It's very common to encounter \"unbalanced\" datasets in HEP, where you might have many more background events available for training compared to the signal.\n",
    "For example in our current dataset, we have double the number of background events.\n",
    "To first order, this actually won't matter at all for our training.\n",
    "This is because the minimum of the standard cross-entropy loss used for classification is independent of the class ratio.\n",
    "In other words, we could have a dataset with a 1:1 class ratio and a 10:1 class ratio, and the weights and biases that minimize the loss function on both datasets would be the same.\n",
    "However, balanced datasets have some nice statistical properties that we can utilize, and truly unbalanced datasets can produce optimization issues.\n",
    "For example, if you class ratio is 1000:1 and your batch size is 256, then most batches will not contain a single signal event!\n",
    "For these reasons, it's typically best to normalize the event weights to our class ratio is one.\n",
    "As an added bonus, we can also normalize the total sum of the event weights across the whole training set such that the mean event weight is 1.0.\n",
    "The following equations make this happen:\n",
    "\n",
    "$$f_s = \\frac{2 \\sum_{i \\in s} w_i}{N_s + N_b}$$\n",
    "$$f_b = \\frac{2 \\sum_{i \\in b} w_i}{N_s + N_b}$$\n",
    "\n",
    "where we can divide the weights for the background and signal class by the factors $f_s$ and $f_b$ respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum of signal weights before normalization:\", weights[target == 1].sum())\n",
    "print(\"Sum of background weights before normalization:\", weights[target == 0].sum(), \"\\n\")\n",
    "\n",
    "f_s = 2 * np.sum(weights[target == 1]) / (len(weights))\n",
    "f_b = 2 * np.sum(weights[target == 0]) / (len(weights))\n",
    "\n",
    "weights[target == 1] /= f_s\n",
    "weights[target == 0] /= f_b\n",
    "\n",
    "print(\"Sum of signal weights after normalization:\", weights[target == 1].sum())\n",
    "print(\"Sum of background weights after normalization:\", weights[target == 0].sum(), \"\\n\")\n",
    "\n",
    "print(\"Class ratio:\", weights[target == 1].sum() / weights[target == 0].sum())\n",
    "print(\"Mean event weight:\", weights.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc46897",
   "metadata": {},
   "source": [
    "At this point we've normalized our weights, have the data and labels ready, and have a pre-processing plan that we'll implement into the network itself.\n",
    "This is a good point at which to turn everything into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: convert all of the numpy arrays to tensors (with the correct dtype)\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"Target shape:\", target.shape)\n",
    "print(\"Weights shape:\", weights.shape)\n",
    "print(\"Means shape:\", means.shape)\n",
    "print(\"Stds shape:\", stds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85e367",
   "metadata": {},
   "source": [
    "This looks almost right.\n",
    "The only issue is that PyTorch will expect the labels and weights to have the same number of dimensions as the data, that is 2.\n",
    "We can add a \"singleton\" dimension using the unsqueeze function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07600c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = target.unsqueeze(1)\n",
    "weights = weights.unsqueeze(1)\n",
    "\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e37a1e9",
   "metadata": {},
   "source": [
    "# How to use a GPU\n",
    "\n",
    "We've already completed a large portion of the work needed to use GPU resources, in that PyTorch should already be recognizing the GPU resources available on your Perlmutter node (`torch.cuda.is_available()` returns True).\n",
    "Getting Pytorch or Tensorflow to recognize and use GPU resources is easier than it used to be, but can still be non-trivial.\n",
    "We won't spend time on it in this tutorial, but you should budget some time to get your software stack setup on whatever resources you will use outside of the school. Ask any of us if you have questions or want advice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed21f0da",
   "metadata": {},
   "source": [
    "We've already seen that PyTorch lets us compute gradients, which is the most essential thing for ML.\n",
    "Another difference between PyTorch and numpy is support for GPU operations.\n",
    "All tensors are by default initialized in CPU memory, even if PyTorch detects GPU resources.\n",
    "You can see what device a tensor is currently stored on using the \"device\" method, and you can move tensors between devices using the \".to\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253c954c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_tensor = torch.randn(3, 4)\n",
    "m2_tensor = torch.randn(4, 3)\n",
    "\n",
    "print(f\"m1 device: {m1_tensor.device}\")\n",
    "print(f\"m2 device: {m2_tensor.device}\")\n",
    "\n",
    "# Move tensors to GPU, note operation is not in place!\n",
    "m1_tensor_gpu = m1_tensor.to(device)\n",
    "m2_tensor_gpu = m2_tensor.to(device)\n",
    "\n",
    "print(f\"m1 device: {m1_tensor_gpu.device}\")\n",
    "print(f\"m2 device: {m2_tensor_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb140f73",
   "metadata": {},
   "source": [
    "That was quick for a small tensor, but in general moving large tensors between devices is not a free operation.\n",
    "Unnecessarily repeating this operation is a common way to accidently bottleneck network training!\n",
    "Ideally you should minimize these operations in your code (or use someone else's code that takes care of this for you).\n",
    "\n",
    "Now that we have tensors on the GPU, let's time our matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First the CPU\n",
    "%timeit m1_tensor @ m2_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5428626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the pytorch version\n",
    "%timeit m1_tensor_gpu @ m2_tensor_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964315f3",
   "metadata": {},
   "source": [
    "It's actually slower! The reason is that GPUs are fast because they parallelize operations, and our matrices are way to small to make that parallelization worthwhile. Let's go bigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b8dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_large = torch.randn(1000, 2000)\n",
    "m2_large = torch.randn(2000, 1000)\n",
    "\n",
    "%timeit m1_large @ m2_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc4b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_large_gpu = m1_large.to(device)\n",
    "m2_large_gpu = m2_large.to(device)\n",
    "\n",
    "%timeit m1_large_gpu @ m2_large_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27578f84",
   "metadata": {},
   "source": [
    "When testing this tutorial, I get roughly a 10x speed-up!\n",
    "That's pretty good in itself, but keep in mind that CPU matrix multiplication is quite optimized. Other operations common in machine learning might enjoy even better speed-ups.\n",
    "\n",
    "These speeds ups will transfer over to the actual training of neural networks.\n",
    "However, as you can hopefully start to understand from the above, the benefits tend to matter most when training large networks on large datasets.\n",
    "If you have a simple problem that be solved perfectly well using small datasets and small networks, you can get a long way with only CPU operations, even run just on your laptop!\n",
    "But if you want to train something state-of-the-art, GPU resources will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aed4a7",
   "metadata": {},
   "source": [
    "Before moving on to building our neural network, let's move all of the data to the GPU.\n",
    "We're doing this here, before splitting the data into different pieces and training a network, to avoid having to perform the \".to\" operation many times.\n",
    "We can get away with this because our data set is very small.\n",
    "If you have to deal with larger data sets, there is a very good chance that your data will not fit in the GPU memory all at once.\n",
    "We won't cover the solutions to this issue today, but there are some links below for further reading.\n",
    "For now, just don't take the fact that we can call these operations with no issues for granted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abee5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: move the data, target, and weight tensors to the GPU\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Target shape: {target.shape}\")\n",
    "print(f\"Weights shape: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b839de",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "It is very common in machine learning to split data into multiple independent sets, and only use part of the data for training/optimizing the machine learning models, and the rest for testing/evaluating performance. This is to check for possible over-fitting.\n",
    "\n",
    "In the following, we will split the whole data into 50% training set, 25% validation set and 25% test set:\n",
    "\n",
    "- __Training Dataset:__ The sample of data used to fit the model.\n",
    "- __Validation Dataset:__ The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.\n",
    "- __Test Dataset:__ The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.\n",
    "\n",
    "\n",
    "We'll do this using the very commonly used `train_test_split` function from Scikit Learn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc74875",
   "metadata": {},
   "source": [
    "### Split the Data into training set and rest (validation + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef10d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    X_val_and_test,\n",
    "    y_train,\n",
    "    y_val_and_test,\n",
    "    weights_train,\n",
    "    weights_val_and_test\n",
    ") = train_test_split(data, target, weights, train_size=0.5)\n",
    "\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"Training Weights shape: \", weights_train.shape, \"\\n\")\n",
    "print(\"X_val_and_test Shape: \", X_val_and_test.shape)\n",
    "print(\"y_val_and_test Shape: \", y_val_and_test.shape)\n",
    "print(\"weights_val_and_test shape: \", weights_val_and_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56734413",
   "metadata": {},
   "source": [
    "### Split the rest (val + test) into validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aee067",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    ") = train_test_split(X_val_and_test, y_val_and_test, weights_val_and_test, train_size=0.5)\n",
    "\n",
    "print(\"X_train Shape: \", X_train.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"weights_train shape: \", weights_train.shape, \"\\n\")\n",
    "print(\"X_val Shape: \", X_val.shape)\n",
    "print(\"y_val Shape: \", y_val.shape)\n",
    "print(\"weights_val shape: \", weights_val.shape, \"\\n\")\n",
    "print(\"X_test Shape: \", X_test.shape)\n",
    "print(\"y_test Shape: \", y_test.shape)\n",
    "print(\"weights_test shape: \", weights_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d743c",
   "metadata": {},
   "source": [
    "## Building the neural network\n",
    "\n",
    "Now for the fun bit.\n",
    "We'll use Pytorch to build a simple multi-layer perceptron neural networks.\n",
    "It will have two hidden layers, with 256 nodes each, interleaves with ReLU activation functions.\n",
    "The last layer uses the Sigmoid activation to output a classifier score that ranges from 0 to 1.\n",
    "\n",
    "In PyTorch, all neural networks are subclasses of the `torch.nn.Module` class.\n",
    "In the `__init__` function (constructor), we'll initialize all of the network's weights and biases.\n",
    "Then we need to define a forward function, which implements the forward pass through the network.\n",
    "Rememeber we also want to make the input pre-processing part of the network as well.\n",
    "We can do all of this in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1546ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiggsNet(torch.nn.Module):\n",
    "    def __init__(self, input_dim, means, stds):\n",
    "        \"\"\"\n",
    "        Initialize the network.\n",
    "        Args:\n",
    "            input_dim (int): The number of input features.\n",
    "            means (torch.Tensor): The mean of the input features, shape (input_dim,).\n",
    "            stds (torch.Tensor): The standard deviation of the input features, shape (input_dim,).\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super().__init__()  # This calls the parent class's constructor\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        # Here we register the means and stds as parameters, but we don't want to train them!\n",
    "        # This is accomplished by setting requires_grad to False.\n",
    "        self.means = nn.Parameter(means, requires_grad=False)\n",
    "        self.stds = nn.Parameter(stds, requires_grad=False)\n",
    "\n",
    "        ## YOUR CODE HERE: define a stack of linear layers with ReLU activations and input dimension input_dim\n",
    "        # self.linear_relu_stack = torch.nn.Sequential(...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        Args:\n",
    "            x (torch.Tensor): The input features, shape (batch_size, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the network, shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "\n",
    "        # Rescale the input features to have mean 0 and std 1\n",
    "        x = (x - self.means) / self.stds\n",
    "\n",
    "        ## YOUR CODE HERE: implement the forward pass through the linear stack and return the output\n",
    "        # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c13e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize the network and print the architecture using the torchinfo library\n",
    "model = HiggsNet(input_dim=6, means=means, stds=stds)\n",
    "torchinfo.summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d36757",
   "metadata": {},
   "source": [
    "Our network has 67,853 total parameters, the vast majority of which are contained in the weights which map between the two hidden layers.\n",
    "Note there are also 12 non-trainable parameters, which are the 6 means and 6 standard deviations that will scale our inputs.\n",
    "Let's see if we can execute a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d95997",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = X_train[:10,:]\n",
    "print(f\"Test input shape: {test_input.shape}\")\n",
    "test_output = model(test_input)\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "print(f\"Test output: {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e110c57",
   "metadata": {},
   "source": [
    "This should return an error, because by default PyTorch initializes the model weights and biases on the CPU, while we have all of our data on the GPU.\n",
    "Go ahead and fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: move the model to the GPU\n",
    "\n",
    "print(f\"Model parameters on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca707091",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model(test_input)\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "print(f\"Test output: {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6eb63",
   "metadata": {},
   "source": [
    "You should see that the network output on some random data points are around 0.5.\n",
    "This is a good place to start for binary classification: the network is guessing equal probability of signal and background, having learned nothing yet.\n",
    "If you don't see this, it's a good sign you haven't scaled your inputs well, or something else is wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be47896",
   "metadata": {},
   "source": [
    "## Training the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c7e666",
   "metadata": {},
   "source": [
    "Before we train, it's a good idea to shuffle the training data.\n",
    "The network would train fine without this step, but sometimes in HEP datasets the data can have hidden order you're not aware of.\n",
    "For example, there can be multiple MC samples pasted together, and if you don't shuffle the network \"sees\" each one of these in sequence as it works through an epoch.\n",
    "Such situations are best avoided.\n",
    "We can shuffle the data using another utility function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ba62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train, weights_train = shuffle(X_train, y_train, weights_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aecb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to use a weighted loss, it's essential to set the reduction to \"none\"\n",
    "# This way we can weight each event individually\n",
    "criterion = torch.nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "# We'll use the Adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Some parameters for the training loop\n",
    "n_epochs = 10\n",
    "batch_size = 1024\n",
    "train_losses = []\n",
    "val_losses_steps = []\n",
    "val_losses = []\n",
    "global_step = 0  # So we can align the validation loss with the training loss\n",
    "\n",
    "# Epoch loop\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "\n",
    "    # Batch loop\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "\n",
    "        # Get a batch of data\n",
    "        X_batch = X_train[i:i+batch_size,:]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "        weights_batch = weights_train[i:i+batch_size]\n",
    "\n",
    "        ## YOUR CODE HERE: implement the forward / backward pass and optimizer step needed to train the model\n",
    "        ## Remember to weight the loss function! There's a hint on how to do this below :)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        # 2. Compute the loss\n",
    "        # 3. Backward pass\n",
    "        # Update the model parameters\n",
    "\n",
    "        # Add the loss to the training loss list\n",
    "        train_losses.append(loss.item())\n",
    "        global_step += 1\n",
    "\n",
    "    # Validation step, we'll do this at the end of each epoch\n",
    "    # Note I won't even both batching this step, since we have plenty of memory, but if you have a big dataset\n",
    "    # you could do this in batches as well.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val)\n",
    "        loss = criterion(outputs, y_val)\n",
    "\n",
    "        # Weight the loss by the event weight, then take mean over the whole validation set\n",
    "        loss = (loss * weights_val).mean()\n",
    "\n",
    "        val_losses_steps.append(global_step)\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "    # As a final step, shuffle the training data again to prepare for the next epoch\n",
    "    ## YOUR CODE HERE: shuffle the training data, labels, and weights\n",
    "    raise NotImplementedError(\"You need to implement this!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c42355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a loss curve\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses_steps, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630d217c",
   "metadata": {},
   "source": [
    "### Use the model to make predicions!\n",
    "Evaluate the model based on predictions made with X_test $\\rightarrow$ y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE: run predictions on the training and testing set\n",
    "# y_pred_test = ...\n",
    "# y_pred_train = ...\n",
    "\n",
    "# Flatten off the singleton dimension on the predictions, labels, and weights\n",
    "# and convert to numpy arrays\n",
    "y_pred_test = y_pred_test.cpu().detach().numpy().flatten()\n",
    "y_pred_train = y_pred_train.cpu().detach().numpy().flatten()\n",
    "y_test = y_test.cpu().detach().numpy().flatten()\n",
    "y_train = y_train.cpu().detach().numpy().flatten()\n",
    "weights_test = weights_test.cpu().detach().numpy().flatten()\n",
    "weights_train = weights_train.cpu().detach().numpy().flatten()\n",
    "\n",
    "# Print the first few predictions and labels\n",
    "with np.printoptions(precision=3, suppress=True, threshold=np.inf):\n",
    "    print(\"y_pred_test: \", y_pred_test[:5])\n",
    "    print(\"y_test: \", y_test[:5].flatten(), \"\\n\")\n",
    "    print(\"y_pred_train: \", y_pred_train[:5])\n",
    "    print(\"y_train: \", y_train[:5].flatten(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97433242",
   "metadata": {},
   "source": [
    "### ROC curves and Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e97c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_true=y_test, y_score=y_pred_test, sample_weight=weights_test)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_true=y_train, y_score=y_pred_train, sample_weight=weights_train)\n",
    "# Note I'm not using the weights here, \"roc_auc_score\" is an example of a function that can break with negative weights\n",
    "auc_test = roc_auc_score(y_true=y_test, y_score=y_pred_test)\n",
    "auc_train = roc_auc_score(y_true=y_train, y_score=y_pred_train)\n",
    "plt.plot(fpr_test, tpr_test, color='tab:blue', lw=2, ls=\"--\", label=f\"Test set auc: {auc_test:.4f}\")\n",
    "plt.plot(fpr_train, tpr_train, color='tab:orange', lw=2, ls=\":\", label=f\"Training set auc: {auc_train:.4f}\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0809a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra_functions import compare_train_test\n",
    "\n",
    "compare_train_test(\n",
    "    y_pred_train,\n",
    "    y_train.flatten(),\n",
    "    y_pred_test,\n",
    "    y_test.flatten(),\n",
    "    xlabel=\"NN Score\",\n",
    "    title=\"NN\",\n",
    "    weights_train=weights_train,\n",
    "    weights_test=weights_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c6b9f",
   "metadata": {},
   "source": [
    "## Significance Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe276d8",
   "metadata": {},
   "source": [
    "In a standard statistical analysis for a search, an observed significance is obtained. This corresponds to the significance with which we reject the background-only hypothesis. That is, the higher the significance, the more likely the signal we are searching for exists.\n",
    "\n",
    "We can use the formula below to obtain an estimated value of the expected significance.\n",
    "\n",
    "$Z = \\sqrt{2+((s+b)\\ln(1+s/b)-s)}$\n",
    "\n",
    "**see [arXiv:1007.1727](https://arxiv.org/pdf/1007.1727.pdf) [Eq. 97]**\n",
    "\n",
    "This corresponds to the signal sensitivity. This is a good indication of the performance of our classification model. => We want the significance to be as high as possible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db82053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from math import log\n",
    "\n",
    "\n",
    "def amsasimov(s, b):\n",
    "    if b <= 0 or s <= 0:\n",
    "        return 0\n",
    "    try:\n",
    "        return sqrt(2 * ((s + b) * log(1 + float(s) / b) - s))\n",
    "    except ValueError:\n",
    "        print(1 + float(s) / b)\n",
    "        print(2 * ((s + b) * log(1 + float(s) / b) - s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79409bcc",
   "metadata": {},
   "source": [
    "To get an idea of useful our classifier is, we can start by calculating the significance for an \"inclusive analysis\" that doesn't cut on the classifier score in order to purify the signal region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0c0088",
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusive_significance = amsasimov(weights_test[y_test == 1].sum(), weights_test[y_test == 0].sum())\n",
    "print(f\"Inclusive significance: {inclusive_significance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3854075e",
   "metadata": {},
   "source": [
    "The simplest way to use the classifier score to separate signal and background is to make a cut on the classifier score for all the events. In the following, we scan through the cut value and look at the corresponding significance. This can help us determine the optimal cut value of the classifier score.\n",
    "Note if we don't cut at all, we obtain the inclusive significance.\n",
    "Any improvement in the significance above the inclusive value indicates that our classifier is useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ab8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_pred_test_sig = [weights_test[(y_test == 1) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "int_pred_test_bkg = [weights_test[(y_test == 0) & (y_pred_test > th_cut)].sum() for th_cut in np.linspace(0, 1, num=50)]\n",
    "vamsasimov = [amsasimov(sumsig, sumbkg) for (sumsig, sumbkg) in zip(int_pred_test_sig, int_pred_test_bkg)]\n",
    "Z = max(vamsasimov)\n",
    "\n",
    "plt.plot(np.linspace(0, 1, num=50), vamsasimov, label=f'Significance ($Z_{{max}} = {np.round(Z, decimals=2)}$)')\n",
    "plt.hlines(inclusive_significance, 0, 1, color=\"red\", linestyle=\"--\", label=\"Inclusive significance\")\n",
    "plt.title(\"NN Significance\")\n",
    "plt.xlabel(\"Cut Value\")\n",
    "plt.ylabel(\"Significance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c77ce4",
   "metadata": {},
   "source": [
    "## Avoiding boilerplate: Pytorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7032b8",
   "metadata": {},
   "source": [
    "If you kept using \"pure pytorch\" in your own work, after some time you'd realize that you spend a lot of time writing the same lines of code over and over again.\n",
    "The perfect example of this is the training loop.\n",
    "It will look very similar for training almost any neural network.\n",
    "This kind of code is called \"boilerplate\", and if you find youself writing a lot of it you're probably wasting time in the writing and inevitable debugging.\n",
    "This is why there are packages that aim to abstract away large chunks of the code needed to train a neural network.\n",
    "Tensorflow, for example, ships with a package called Keras that does this.\n",
    "The JAX tools are less developed, but are getting better and better.\n",
    "For pytorch, there is a package called PyTorch lightning.\n",
    "\n",
    "Let's re-run training the network, but do it using lightning and see how much more efficient we can be.\n",
    "Along the way, we'll also use some data utility classes that ship with PyTorch, and automate a lot of the data handling steps we had to implement ourselves above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a5c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2063d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central to pytorch lightning is the \"LightningModule\" class which organizes steps in the training loop into reusable pieces\n",
    "from numpy.lib.polynomial import _raise_power\n",
    "\n",
    "\n",
    "class HiggsNetLightning(L.LightningModule):\n",
    "    def __init__(self, input_dim, means, stds):\n",
    "        super().__init__()\n",
    "        self.model = HiggsNet(input_dim=input_dim, means=means, stds=stds)\n",
    "        self.criterion = torch.nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, weights = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        loss = (loss * weights).mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)  # Display the loss in the progress bar\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ## YOUR CODE HERE: implement the validation step\n",
    "        raise NotImplementedError(\"You need to implement this!\")\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)  # Display the loss in the progress bar\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        ## YOUR CODE HERE: run predictions with the model\n",
    "        # y_hat = ...\n",
    "        raise NotImplementedError(\"You need to implement this!\")\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa14e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HiggsNetLightning(input_dim=6, means=means, stds=stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c6e0d",
   "metadata": {},
   "source": [
    "If we're not going to write a training loop, what do we do with the data? Lightning expects the data to be wrapper in pytorch datasets and dataloaders, two classes that make training neural networks vastly easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9960914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tensor dataset is a simple class that wraps a tensor and provides a way to iterate over it\n",
    "dataset = torch.utils.data.TensorDataset(data, target, weights)\n",
    "\n",
    "# Datasets support splitting into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.5, 0.25, 0.25])\n",
    "\n",
    "# Datasets can be used in a dataloader, which will automatically batch and shuffle the data\n",
    "# We also don't need to use \".to(device)\" here, as the dataloader will automatically move the data to the GPU\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)  # Setting shuffle to True will shuffle the data every epoch\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c09e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some operations to get the data out of the dataset and dataloader classes\n",
    "first_five_events = dataset[:5]  # This yields a tuple of tensors, in the order they were passed to the dataset\n",
    "print(f\"Data: {first_five_events[0].shape}\")\n",
    "print(f\"Target: {first_five_events[1].shape}\")\n",
    "print(f\"Weights: {first_five_events[2].shape}\", \"\\n\")\n",
    "\n",
    "# To get data out of the dataloader, you can iterate over it, or if you want one batch, build an iterable and use the \"next\" function\n",
    "first_batch = next(iter(train_loader))\n",
    "print(f\"Data: {first_batch[0].shape}\")\n",
    "print(f\"Target: {first_batch[1].shape}\")\n",
    "print(f\"Weights: {first_batch[2].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6fa0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last thing we need is the \"Trainer\" class, which handles the training, validation, and testing of the model\n",
    "trainer = L.Trainer(max_epochs=10, accelerator=\"gpu\", devices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58d04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run training with one line of code\n",
    "# Note we also don't need to move the model to GPU, lightning does this for us\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b8f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run predictions, we can also use the trainer\n",
    "predictions = trainer.predict(model, test_loader)\n",
    "\n",
    "# Predictions are returned as a list of tensors, so we need to concatenate them\n",
    "predictions = np.concatenate([p.cpu().detach().numpy().flatten() for p in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally plot a ROC curve and calculate AUC\n",
    "y_test = test_dataset[:][1].cpu().detach().numpy().flatten()\n",
    "weights_test = test_dataset[:][2].cpu().detach().numpy().flatten()\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_true=y_test, y_score=predictions, sample_weight=weights_test)\n",
    "auc_test = roc_auc_score(y_true=y_test, y_score=predictions)\n",
    "plt.plot(fpr_test, tpr_test, color='tab:blue', lw=2, ls=\"--\", label=f\"Test set auc: {auc_test:.4f}\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e25bd",
   "metadata": {},
   "source": [
    "As a final note, for simple tasks like this one using all of these classes might seem a little silly, it's not that much more work to just do it yourself.\n",
    "However when you start to scale things to solve research problems, things can get complicated very quickly.\n",
    "Some advanced topics that these packages will vastly simplify, at least in standard applications:\n",
    "\n",
    "- Parallelized data loading: https://docs.pytorch.org/docs/stable/data.html\n",
    "- Experiment tracking: https://wandb.ai/site/ or https://www.comet.com/site/\n",
    "- Data parallel training: https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html\n",
    "- Model parallel training (god mode): https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html\n",
    "\n",
    "In general, the more code you can offload onto someone else, the more time and frustration you'll save yourself.\n",
    "Packages are very helpful, use them as much as possible!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c9403",
   "metadata": {},
   "source": [
    "Some exercises to try if there's time:\n",
    "\n",
    "1. Try to get a higher AUC. Some ideas:\n",
    "- Adjust learning rate\n",
    "- Adjust the number of neurons, and layers\n",
    "- Increase epochs\n",
    "- Enable early stopping (and increase max epochs)\n",
    "- Adjust batch size\n",
    "- Change activations: [relu, leakyrelu, selu, tanh, gelu]\n",
    "- Try different optimizers (e.g. AdamW, Ranger, Lion)\n",
    "- Learning rate scheduler\n",
    "- Use a more complicated structure? This data is \"tabular\", meaning we don't have variable length inputs, so maybe don't go there just yet\n",
    "- More... google!\n",
    "\n",
    "2. Break the model. Some ideas:\n",
    "- Get it to overfit\n",
    "- Don't standardize the inputs\n",
    "- Use very unbalanced data\n",
    "- Mess with the weights\n",
    "\n",
    "3. Train a BDT instead\n",
    "- Lightning fast to train, and very very good at simple tasks\n",
    "- Would definitely cover this, if we had more time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lbl-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
