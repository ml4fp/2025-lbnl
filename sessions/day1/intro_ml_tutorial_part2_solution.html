
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Binary classification on a HEP dataset &#8212; ML4FP 2025</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"TeX": {"Macros": {"vector": ["\\vec{#1}", 1], "uvec": ["\\hat{#1}", 1], "mag": ["\\lVert#1\\rVert", 1], "cross": "\\times", "unit": ["#1~\\mathrm{#2}", 2]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'sessions/day1/intro_ml_tutorial_part2_solution';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../home.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/ml4fp_2025_logo.png" class="logo__image only-light" alt="ML4FP 2025 - Home"/>
    <script>document.write(`<img src="../../_static/ml4fp_2025_logo.png" class="logo__image only-dark" alt="ML4FP 2025 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../home.html">
                    Machine Learning for Fundamental Physics (ML4FP) School 2025
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Logistics</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../pages/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pages/registration.html">Registration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pages/code-of-conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../pages/schedule.html">Schedule</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="day1.html">Intro to ML</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_ml_tutorial_part1.html">Introduction to PyTorch and binary classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro_ml_tutorial_part2.html">Binary classification on a HEP dataset</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../day2/day2.html">Day 2: ML Overviews, Differentiable Programing and NSBI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../day3/day3.html">Day 3: Transformers and Foundation Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../day3/transformers_tutorial_part1.html">Transformers Tutorial: Part I</a></li>
<li class="toctree-l1"><a class="reference internal" href="../day3/transformers_tutorial_part2.html">Transformers Tutorial: Part II</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../day4/day4.html">Day 4: Generative Models and Anomaly Detection</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Day 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../day5/day5.html">Day 5: Experiment-specific Sessions and Efficient ML</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/ml4fp/2025-lbnl" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/sessions/day1/intro_ml_tutorial_part2_solution.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Binary classification on a HEP dataset</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Binary classification on a HEP dataset</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-data">Load data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#event-selection">Event selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-the-signal-background-weights-for-balanced-training">Adjust the Signal/Background Weights for balanced training</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-a-gpu">How to use a GPU</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">Data splitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-the-data-into-training-set-and-rest-validation-test">Split the Data into training set and rest (validation + test)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-the-rest-val-test-into-validation-set-and-test-set">Split the rest (val + test) into validation set and test set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-neural-network">Building the neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-neural-network">Training the neural network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-model-to-make-predicions">Use the model to make predicions!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves-and-area-under-the-curve-auc">ROC curves and Area Under the Curve (AUC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-function">Significance Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-boilerplate-pytorch-lightning">Avoiding boilerplate: Pytorch lightning</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="binary-classification-on-a-hep-dataset">
<h1>Binary classification on a HEP dataset<a class="headerlink" href="#binary-classification-on-a-hep-dataset" title="Link to this heading">#</a></h1>
<p>In the tutorial we’ll set aside the fundamentals and focus on the practical stuff: how to train a deep neural network on a HEP dataset.
Our problem will still be binary classification: distinguishing between signal and background.
This is a simple and very common problem in HEP, which can also be surprisingly subtle.
Entire working groups within experimental collaborations are dedicated to just this task.</p>
<p>The dataset is from collider physics, but the methods and typical problems you might encounter should carry over to data from other HEP contexts, e.g. neutrino experiments.</p>
<p>We’ll use pytorch again. If you plan to use tensorflow or JAX for your own work, translate the notebook as an exercise!</p>
<p>Outline:</p>
<ul class="simple">
<li><p>Load data from root files</p></li>
<li><p>Explore the data and weights</p></li>
<li><p>Preprocess data for training</p></li>
<li><p>How to use GPU resources</p></li>
<li><p>Train / test / val splits</p></li>
<li><p>Train a neural network</p></li>
<li><p>Quantify its performance</p></li>
<li><p>Tune the model and overtraining</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uproot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tqdm</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchinfo</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We&#39;ll use GPU acceleration in this tutorial. Let&#39;s check if we have one available.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Is CUDA available? </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of GPUs available: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Is CUDA available? True
Number of GPUs available: 4
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You can change the &quot;0&quot; here to 1, 2, or 3 to use other GPUs on the node for the rest of the notebook</span>
<span class="n">dnumber</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">dnumber</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">device_name</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_name</span><span class="p">(</span><span class="n">dnumber</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device name: </span><span class="si">{</span><span class="n">device_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using device: cuda:0
Device name: NVIDIA A100-SXM4-40GB
</pre></div>
</div>
</div>
</div>
<section id="load-data">
<h2>Load data<a class="headerlink" href="#load-data" title="Link to this heading">#</a></h2>
<p>This data set was produced from the ATLAS open data 2020 release:</p>
<p><a class="reference external" href="https://opendata.atlas.cern/docs/category/13-tev-2020-release">https://opendata.atlas.cern/docs/category/13-tev-2020-release</a></p>
<p>The dataset contains MC simulations of the following processes that occur within LHC collisions:</p>
<ul class="simple">
<li><p>Higgs boson production, where the Higgs decays to a pair of W boson that then decay leptonically: <span class="math notranslate nohighlight">\(H \rightarrow WW \rightarrow l \nu l \nu\)</span>. We want to isolate this signal so we can study this Higgs boson.</p></li>
<li><p>Other standard model processes: top-quark-pair production, single-top production, electroweak boson production in association with jets (W+jets, Z+jets), and diboson production (WW, WZ, ZZ). Jets are a columnated spray of hadrons that are the experimental signature of a quark or gluon produced with large transverse momentum. All of these processes can also result in final states with two leptons and significant missing transverse momentum, making them backgrounds. If we define some signal region that should contain <span class="math notranslate nohighlight">\(H \rightarrow WW \rightarrow l \nu l \nu\)</span> events, events some from of these background processes will contaminate it and degrade the sensitivity of our result. So, we want to remove these backgrounds.</p></li>
</ul>
<p>The binary classification task will be to discriminate signal (<span class="math notranslate nohighlight">\(H \rightarrow WW\)</span>) from background (all of the other SM processes).
Our data will be a list of kinematic features that characterize each event.
These features should be somewhat different between the signal and background in non-trivial ways, and our classifier will use these differences to separate the two classes.</p>
<p>All the data are stored in a root file, which contains different kinematic variables of each event, the event label (<code class="docutils literal notranslate"><span class="pre">label</span></code>) to indicate whether an event is a signal or background, as well as the MC event weights (<code class="docutils literal notranslate"><span class="pre">mcWeight</span></code>). We use <a class="reference external" href="https://uproot.readthedocs.io/en/latest/">uproot</a> to read data from the root file.
Given the prevalence of root files in HEP, and the prevalence of Python in machine learning, uproot is an indispensible tool for applications of ML in HEP.
Learn to use it!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;dataWW_d1.root&quot;</span>
<span class="n">file</span> <span class="o">=</span> <span class="n">uproot</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

<span class="c1"># show what is inside the root file loaded from uproot</span>
<span class="nb">print</span><span class="p">(</span><span class="n">file</span><span class="o">.</span><span class="n">classnames</span><span class="p">())</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">file</span><span class="p">[</span><span class="s2">&quot;tree_event&quot;</span><span class="p">]</span>  <span class="c1"># select the TTree inside the root file</span>
<span class="n">tree</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  <span class="c1"># show all the branches inside the TTree</span>
<span class="n">dfall</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span><span class="n">library</span><span class="o">=</span><span class="s2">&quot;pd&quot;</span><span class="p">)</span>  <span class="c1"># convert uproot TTree into pandas dataframe</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;============================================&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;File loaded with &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dfall</span><span class="p">),</span> <span class="s2">&quot; events &quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;tree_event;1&#39;: &#39;TTree&#39;}
name                 | typename                 | interpretation                
---------------------+--------------------------+-------------------------------
index                | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
eventNumber          | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
label                | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
met_et               | double                   | AsDtype(&#39;&gt;f8&#39;)
met_phi              | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_n                | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
lep_pt_0             | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_pt_1             | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_eta_0            | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_eta_1            | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_phi_0            | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_phi_1            | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_E_0              | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_E_1              | double                   | AsDtype(&#39;&gt;f8&#39;)
lep_charge_0         | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
lep_charge_1         | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
lep_type_0           | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
lep_type_1           | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
jet_n                | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
jet_pt_0             | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_pt_1             | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_eta_0            | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_eta_1            | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_phi_0            | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_phi_1            | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_E_0              | double                   | AsDtype(&#39;&gt;f8&#39;)
jet_E_1              | double                   | AsDtype(&#39;&gt;f8&#39;)
mcWeight             | double                   | AsDtype(&#39;&gt;f8&#39;)
runNumber            | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
channelNumber        | int64_t                  | AsDtype(&#39;&gt;i8&#39;)
============================================
File loaded with  600000  events 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dump list of feature</span>
<span class="n">dfall</span><span class="o">.</span><span class="n">columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Index([&#39;index&#39;, &#39;eventNumber&#39;, &#39;label&#39;, &#39;met_et&#39;, &#39;met_phi&#39;, &#39;lep_n&#39;,
       &#39;lep_pt_0&#39;, &#39;lep_pt_1&#39;, &#39;lep_eta_0&#39;, &#39;lep_eta_1&#39;, &#39;lep_phi_0&#39;,
       &#39;lep_phi_1&#39;, &#39;lep_E_0&#39;, &#39;lep_E_1&#39;, &#39;lep_charge_0&#39;, &#39;lep_charge_1&#39;,
       &#39;lep_type_0&#39;, &#39;lep_type_1&#39;, &#39;jet_n&#39;, &#39;jet_pt_0&#39;, &#39;jet_pt_1&#39;,
       &#39;jet_eta_0&#39;, &#39;jet_eta_1&#39;, &#39;jet_phi_0&#39;, &#39;jet_phi_1&#39;, &#39;jet_E_0&#39;,
       &#39;jet_E_1&#39;, &#39;mcWeight&#39;, &#39;runNumber&#39;, &#39;channelNumber&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># examine first few events</span>
<span class="n">dfall</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>index</th>
      <th>eventNumber</th>
      <th>label</th>
      <th>met_et</th>
      <th>met_phi</th>
      <th>lep_n</th>
      <th>lep_pt_0</th>
      <th>lep_pt_1</th>
      <th>lep_eta_0</th>
      <th>lep_eta_1</th>
      <th>...</th>
      <th>jet_pt_1</th>
      <th>jet_eta_0</th>
      <th>jet_eta_1</th>
      <th>jet_phi_0</th>
      <th>jet_phi_1</th>
      <th>jet_E_0</th>
      <th>jet_E_1</th>
      <th>mcWeight</th>
      <th>runNumber</th>
      <th>channelNumber</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>249632</td>
      <td>1</td>
      <td>25.118</td>
      <td>1.14010</td>
      <td>2</td>
      <td>46.439</td>
      <td>44.589</td>
      <td>0.131030</td>
      <td>0.40654</td>
      <td>...</td>
      <td>-7.000</td>
      <td>2.4346</td>
      <td>-7.00000</td>
      <td>-0.76667</td>
      <td>-7.00000</td>
      <td>308720.0</td>
      <td>-7.0</td>
      <td>0.000002</td>
      <td>284500</td>
      <td>345323</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>892975</td>
      <td>1</td>
      <td>27.974</td>
      <td>0.84442</td>
      <td>2</td>
      <td>86.819</td>
      <td>55.438</td>
      <td>-0.017265</td>
      <td>-0.16338</td>
      <td>...</td>
      <td>-7.000</td>
      <td>-1.2789</td>
      <td>-7.00000</td>
      <td>1.66290</td>
      <td>-7.00000</td>
      <td>245670.0</td>
      <td>-7.0</td>
      <td>0.000002</td>
      <td>284500</td>
      <td>345323</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>730573</td>
      <td>0</td>
      <td>123.290</td>
      <td>-2.94810</td>
      <td>2</td>
      <td>105.810</td>
      <td>40.506</td>
      <td>-1.338900</td>
      <td>-1.03500</td>
      <td>...</td>
      <td>-7.000</td>
      <td>-7.0000</td>
      <td>-7.00000</td>
      <td>-7.00000</td>
      <td>-7.00000</td>
      <td>-7.0</td>
      <td>-7.0</td>
      <td>0.002468</td>
      <td>284500</td>
      <td>363492</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>717584</td>
      <td>0</td>
      <td>78.558</td>
      <td>1.04450</td>
      <td>2</td>
      <td>56.666</td>
      <td>35.124</td>
      <td>2.294600</td>
      <td>1.88700</td>
      <td>...</td>
      <td>53.942</td>
      <td>1.6270</td>
      <td>0.55314</td>
      <td>-1.84880</td>
      <td>-0.47055</td>
      <td>211610.0</td>
      <td>63629.0</td>
      <td>0.000197</td>
      <td>284500</td>
      <td>363492</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>319020</td>
      <td>0</td>
      <td>15.863</td>
      <td>0.74840</td>
      <td>2</td>
      <td>94.708</td>
      <td>72.811</td>
      <td>0.812740</td>
      <td>0.77954</td>
      <td>...</td>
      <td>-7.000</td>
      <td>-0.6358</td>
      <td>-7.00000</td>
      <td>-1.91230</td>
      <td>-7.00000</td>
      <td>173810.0</td>
      <td>-7.0</td>
      <td>0.000197</td>
      <td>284500</td>
      <td>363492</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 30 columns</p>
</div></div></div>
</div>
<p>We have 800k events in this data file, which sits in a nice medium range where we have plenty of data for training but not so much data that we will run into memory constraints.
What to do in cases when this is not true is beyond the scope for today, but keep in mind it didn’t need to be this way, and often won’t be in real world applications.</p>
</section>
<section id="event-selection">
<h2>Event selection<a class="headerlink" href="#event-selection" title="Link to this heading">#</a></h2>
<p>In this example, we want to focus on events with exactly 2 leptons (electrons or muons).
There are both signal and background events where this will not be the case, e.g. if a lepton is missed by the detector, but this is a small effect that we can ignore for today.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of events before selections:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">dfall</span><span class="p">))</span>
<span class="n">fulldata</span> <span class="o">=</span> <span class="n">dfall</span><span class="p">[</span><span class="n">dfall</span><span class="o">.</span><span class="n">lep_n</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of events after selections:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fulldata</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of events before selections: 600000
Number of events after selections: 596571
</pre></div>
</div>
</div>
</div>
<p>In collider physics, a common feature of simulated datasets is event weights.
These weights arise from the beyond leading order Feynman diagram calculations that occur within event generators.
Without the weights, the simulation will not resemble real data with as much accuracy.
If you’re a collider physicist, you’re probably already familiar with these weights.
If not, you can ignore this detail and just continue on in the notebook.
As you’ll see, what we’ll have to do to handle these weights is actually quite minimal</p>
<p>Let’s load the weights, stored in the column <code class="docutils literal notranslate"><span class="pre">mcWeight</span></code> and the labels, stored in the column <code class="docutils literal notranslate"><span class="pre">target</span></code>. Like in this morning’s tutorial, the labels will take on a value of 1.0 for signal and 0.0 for background</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fulldata</span><span class="o">.</span><span class="n">label</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">fulldata</span><span class="o">.</span><span class="n">mcWeight</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of selected signal events:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fulldata</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of selected background events:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">fulldata</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]))</span>

<span class="c1"># plot the distribution of the weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Event weight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Number of events&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean weight: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Std weight: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max weight: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Min weight: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of selected signal events: 397728
Number of selected background events: 198843
</pre></div>
</div>
<img alt="../../_images/d661b83d67365cdd2fe1f240d9ec71c76cd09b36724ed17345909062dbbd302d.png" src="../../_images/d661b83d67365cdd2fe1f240d9ec71c76cd09b36724ed17345909062dbbd302d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean weight: 0.00021820649947751399
Std weight: 0.0007363346766555201
Max weight: 0.05242
Min weight: -0.045557
</pre></div>
</div>
</div>
</div>
<p>For the collider physicists: a few things to note about this weight distribution. First, the mean weight is very small, only about 0.0002. This reduces the effective number of events in our data sample, but as we’ll see it doesn’t actually matter for training the network. Second, some of these weights are negative. This may seem like a problem, and many data science tools will break if asked to work with some negative event weights, but in reality we can train networks with a small fraction of negative weights with no issue, as we’ll do so here!</p>
<p>Next we need to decide which features we’ll use to disciminate the signal from background. Tho start we’ll just use 6 features, but one of the great things about machine learning based classifiers, like neural networks or BDTs, is they can easily accommodate and will benefit from more features, assuming those features are pre-processed properly. These extra features can be either orthogonal features that tell the network something new about the events, or “engineered” features which we know, perhaps from thinking about the underlying physics, are useful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">fulldata</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;met_et&quot;</span><span class="p">,</span> <span class="s2">&quot;met_phi&quot;</span><span class="p">,</span> <span class="s2">&quot;lep_pt_0&quot;</span><span class="p">,</span> <span class="s2">&quot;lep_pt_1&quot;</span><span class="p">,</span> <span class="s1">&#39;lep_eta_0&#39;</span><span class="p">,</span> <span class="s1">&#39;lep_eta_1&#39;</span><span class="p">])</span>

<span class="c1"># Or we can use more features to improve the discrimination power:</span>
<span class="c1"># data=pd.DataFrame(fulldata, columns=[&quot;met_et&quot;,&quot;met_phi&quot;,&quot;lep_pt_0&quot;,&quot;lep_pt_1&quot;,&#39;lep_eta_0&#39;, &#39;lep_eta_1&#39;,</span>
<span class="c1">#                                      &#39;lep_phi_0&#39;, &#39;lep_phi_1&#39;,&#39;jet_n&#39;,&#39;jet_pt_0&#39;,</span>
<span class="c1">#                                      &#39;jet_pt_1&#39;, &#39;jet_eta_0&#39;, &#39;jet_eta_1&#39;, &#39;jet_phi_0&#39;, &#39;jet_phi_1&#39;])</span>

<span class="c1"># We can also engineer our own feature:</span>
<span class="c1"># data[&quot;lep_deltaphi&quot;]=np.abs(np.mod(data.lep_phi_1-data.lep_phi_0+3*np.pi,2*np.pi)-np.pi)</span>

<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>met_et</th>
      <th>met_phi</th>
      <th>lep_pt_0</th>
      <th>lep_pt_1</th>
      <th>lep_eta_0</th>
      <th>lep_eta_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.118</td>
      <td>1.14010</td>
      <td>46.439</td>
      <td>44.589</td>
      <td>0.131030</td>
      <td>0.40654</td>
    </tr>
    <tr>
      <th>1</th>
      <td>27.974</td>
      <td>0.84442</td>
      <td>86.819</td>
      <td>55.438</td>
      <td>-0.017265</td>
      <td>-0.16338</td>
    </tr>
    <tr>
      <th>2</th>
      <td>123.290</td>
      <td>-2.94810</td>
      <td>105.810</td>
      <td>40.506</td>
      <td>-1.338900</td>
      <td>-1.03500</td>
    </tr>
    <tr>
      <th>3</th>
      <td>78.558</td>
      <td>1.04450</td>
      <td>56.666</td>
      <td>35.124</td>
      <td>2.294600</td>
      <td>1.88700</td>
    </tr>
    <tr>
      <th>4</th>
      <td>15.863</td>
      <td>0.74840</td>
      <td>94.708</td>
      <td>72.811</td>
      <td>0.812740</td>
      <td>0.77954</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What are these features?</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">met_et</span></code> is the missing transverse momentum in the event in units of GeV. We expect to see a large amount of missing transverse momentum in the signal class, because of the two neutrinos in the final state.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lep_pt_0</span></code> and <code class="docutils literal notranslate"><span class="pre">lep_pt_1</span></code> are the transverse momentum of the leading (higher <span class="math notranslate nohighlight">\(p_T\)</span>) and sub-leading (lower <span class="math notranslate nohighlight">\(p_T\)</span>) leptons in units of GeV.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">met_phi</span></code> is the azimuthal angle of the missing transverse momentum vector</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lep_eta_0</span></code> and <code class="docutils literal notranslate"><span class="pre">lep_eta_1</span></code> are the pseudorapidity of the leading and sub-leading leptons, respectively</p></li>
</ul>
<p>If you’re unfamiliar with these coordinates, see the Figure below (pseudorapidity is denoted <span class="math notranslate nohighlight">\(\eta\)</span>).
Note the azimuthal angle observable offers no separation between signal and background, as we could expect from symmetry arguments.
You can get rid of it from what follows and expect no differences in performance, but we can also include it and see no drop in performance.
In general, neural networks and BDT based binary classifiers are very good at ignoring irrelevant features in most applications.
There’s generally no risk to adding extra features as input.
If you’re tring to maximize performance, the best advice is to use everything!</p>
<img src="axis3D_CMS-005.png" alt="drawing" width="700"/><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;signal&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()[:</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># to avoid error if holes in the grid of plots (like if 7 or 8 features)</span>
<span class="n">data</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span>
    <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;background&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ax</span><span class="p">)):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;right&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="s2">&quot;phi&quot;</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">keys</span><span class="p">()[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 640x480 with 0 Axes&gt;
</pre></div>
</div>
<img alt="../../_images/3fdf6523c3e05cb728e672ed4dda74cb7175d6cb6aed6bf5dc4820a5150dc9df.png" src="../../_images/3fdf6523c3e05cb728e672ed4dda74cb7175d6cb6aed6bf5dc4820a5150dc9df.png" />
</div>
</div>
</section>
<section id="data-preprocessing">
<h2>Data preprocessing<a class="headerlink" href="#data-preprocessing" title="Link to this heading">#</a></h2>
<p>The scale of the inputs we will use as input to our network vary significantly.
For example the dimensionful inputs can have values up to several thousand, but the azimuthal angle and pseudorapidity inputs are bounded and symmetrically distributed about zero.
In network training, large scale inputs can have significant negative impacts, making the optimization either very difficult, or causing it to break entirely.
It’s always best to make histograms of your inputs prior to training, and ensure that they have some “reasonable” distribution…</p>
<p>These are the 2 most common ways to rescale a given input:</p>
<ol class="arabic simple">
<li><p><strong>Scale to Mean of 0 and Variance of 1.0:</strong>   <span class="math notranslate nohighlight">\(\ \ \ \ x^\prime = (x-\mu)/\sigma\)</span></p></li>
<li><p><strong>Scale to Max of 1 and Min of 0:</strong>   <span class="math notranslate nohighlight">\(\ \ \ \ x^\prime = (x-x_{\mathrm{min}})/(x_{\mathrm{max}}-x_{\mathrm{min}})\)</span></p></li>
</ol>
<p>In the following, we’ll take the first approach. One way to do this would be to apply the scaling right now, to all of the data, and forget about it for the rest of the notebook.
However I personally dislike this approach because if you ever want to use the trained network later, you’ll need to remember exactly what scaling you applied to the data.
This is a great way to introduce a hard to find bug.
Instead, I find it useful to make the rescaling a part of the network itself.
We’ll do this below.
All we need to do for now is calculate the mean and std. deviation of our input observables.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Means:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stds:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stds</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Means:
[ 6.02780576e+01 -8.23192340e-03  6.01928220e+01  2.81181012e+01
  4.10741639e-04 -6.73525301e-04]
Stds:
[46.61779439  1.81266806 40.94265249 20.34724452  1.21066976  1.23602892]
</pre></div>
</div>
</div>
</div>
</section>
<section id="adjust-the-signal-background-weights-for-balanced-training">
<h2>Adjust the Signal/Background Weights for balanced training<a class="headerlink" href="#adjust-the-signal-background-weights-for-balanced-training" title="Link to this heading">#</a></h2>
<p>It’s very common to encounter “unbalanced” datasets in HEP, where you might have many more background events available for training compared to the signal.
For example in our current dataset, we have double the number of background events.
To first order, this actually won’t matter at all for our training.
This is because the minimum of the standard cross-entropy loss used for classification is independent of the class ratio.
In other words, we could have a dataset with a 1:1 class ratio and a 10:1 class ratio, and the weights and biases that minimize the loss function on both datasets would be the same.
However, balanced datasets have some nice statistical properties that we can utilize, and truly unbalanced datasets can produce optimization issues.
For example, if you class ratio is 1000:1 and your batch size is 256, then most batches will not contain a single signal event!
For these reasons, it’s typically best to normalize the event weights to our class ratio is one.
As an added bonus, we can also normalize the total sum of the event weights across the whole training set such that the mean event weight is 1.0.
The following equations make this happen:</p>
<div class="math notranslate nohighlight">
\[f_s = \frac{2 \sum_{i \in s} w_i}{N_s + N_b}\]</div>
<div class="math notranslate nohighlight">
\[f_b = \frac{2 \sum_{i \in b} w_i}{N_s + N_b}\]</div>
<p>where we can divide the weights for the background and signal class by the factors <span class="math notranslate nohighlight">\(f_s\)</span> and <span class="math notranslate nohighlight">\(f_b\)</span> respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of signal weights before normalization:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of background weights before normalization:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">f_s</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>
<span class="n">f_b</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">))</span>

<span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span> <span class="o">/=</span> <span class="n">f_s</span>
<span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">/=</span> <span class="n">f_b</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of signal weights after normalization:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum of background weights after normalization:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Class ratio:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">weights</span><span class="p">[</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean event weight:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sum of signal weights before normalization: 6.166582097600001
Sum of background weights before normalization: 124.0090875022 

Sum of signal weights after normalization: 298285.5
Sum of background weights after normalization: 298285.5 

Class ratio: 1.0
Mean event weight: 0.9999999999999998
</pre></div>
</div>
</div>
</div>
<p>At this point we’ve normalized our weights, have the data and labels ready, and have a pre-processing plan that we’ll implement into the network itself.
This is a good point at which to turn everything into PyTorch tensors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">target</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">stds</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data shape:&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Target shape:&quot;</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Weights shape:&quot;</span><span class="p">,</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Means shape:&quot;</span><span class="p">,</span> <span class="n">means</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Stds shape:&quot;</span><span class="p">,</span> <span class="n">stds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data shape: torch.Size([596571, 6])
Target shape: torch.Size([596571])
Weights shape: torch.Size([596571])
Means shape: torch.Size([6])
Stds shape: torch.Size([6])
</pre></div>
</div>
</div>
</div>
<p>This looks almost right.
The only issue is that PyTorch will expect the labels and weights to have the same number of dimensions as the data, that is 2.
We can add a “singleton” dimension using the unsqueeze function</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target shape: </span><span class="si">{</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights shape: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Target shape: torch.Size([596571, 1])
Weights shape: torch.Size([596571, 1])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="how-to-use-a-gpu">
<h1>How to use a GPU<a class="headerlink" href="#how-to-use-a-gpu" title="Link to this heading">#</a></h1>
<p>We’ve already completed a large portion of the work needed to use GPU resources, in that PyTorch should already be recognizing the GPU resources available on your Perlmutter node (<code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code> returns True).
Getting Pytorch or Tensorflow to recognize and use GPU resources is easier than it used to be, but can still be non-trivial.
We won’t spend time on it in this tutorial, but you should budget some time to get your software stack setup on whatever resources you will use outside of the school. Ask any of us if you have questions or want advice!</p>
<p>We’ve already seen that PyTorch lets us compute gradients, which is the most essential thing for ML.
Another difference between PyTorch and numpy is support for GPU operations.
All tensors are by default initialized in CPU memory, even if PyTorch detects GPU resources.
You can see what device a tensor is currently stored on using the “device” method, and you can move tensors between devices using the “.to” method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">m2_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;m1 device: </span><span class="si">{</span><span class="n">m1_tensor</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;m2 device: </span><span class="si">{</span><span class="n">m2_tensor</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Move tensors to GPU, note operation is not in place!</span>
<span class="n">m1_tensor_gpu</span> <span class="o">=</span> <span class="n">m1_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">m2_tensor_gpu</span> <span class="o">=</span> <span class="n">m2_tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;m1 device: </span><span class="si">{</span><span class="n">m1_tensor_gpu</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;m2 device: </span><span class="si">{</span><span class="n">m2_tensor_gpu</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>m1 device: cpu
m2 device: cpu
m1 device: cuda:0
m2 device: cuda:0
</pre></div>
</div>
</div>
</div>
<p>That was quick for a small tensor, but in general moving large tensors between devices is not a free operation.
Unnecessarily repeating this operation is a common way to accidently bottleneck network training!
Ideally you should minimize these operations in your code (or use someone else’s code that takes care of this for you).</p>
<p>Now that we have tensors on the GPU, let’s time our matrix multiplication</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># First the CPU</span>
<span class="o">%</span><span class="k">timeit</span> m1_tensor @ m2_tensor
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.62 μs ± 13.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now the pytorch version</span>
<span class="o">%</span><span class="k">timeit</span> m1_tensor_gpu @ m2_tensor_gpu
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7.71 μs ± 264 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>It’s actually slower! The reason is that GPUs are fast because they parallelize operations, and our matrices are way to small to make that parallelization worthwhile. Let’s go bigger</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">2000</span><span class="p">)</span>
<span class="n">m2_large</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="o">%</span><span class="k">timeit</span> m1_large @ m2_large
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2.61 ms ± 369 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">m1_large_gpu</span> <span class="o">=</span> <span class="n">m1_large</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">m2_large_gpu</span> <span class="o">=</span> <span class="n">m2_large</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="o">%</span><span class="k">timeit</span> m1_large_gpu @ m2_large_gpu
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>235 μs ± 4.12 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
</pre></div>
</div>
</div>
</div>
<p>When testing this tutorial, I get roughly a 10x speed-up!
That’s pretty good in itself, but keep in mind that CPU matrix multiplication is quite optimized. Other operations common in machine learning might enjoy even better speed-ups.</p>
<p>These speeds ups will transfer over to the actual training of neural networks.
However, as you can hopefully start to understand from the above, the benefits tend to matter most when training large networks on large datasets.
If you have a simple problem that be solved perfectly well using small datasets and small networks, you can get a long way with only CPU operations, even run just on your laptop!
But if you want to train something state-of-the-art, GPU resources will be required.</p>
<p>Before moving on to building our neural network, let’s move all of the data to the GPU.
We’re doing this here, before splitting the data into different pieces and training a network, to avoid having to perform the “.to” operation many times.
We can get away with this because our data set is very small.
If you have to deal with larger data sets, there is a very good chance that your data will not fit in the GPU memory all at once.
We won’t cover the solutions to this issue today, but there are some links below for further reading.
For now, just don’t take the fact that we can call these operations with no issues for granted!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data shape: </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target shape: </span><span class="si">{</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights shape: </span><span class="si">{</span><span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data shape: torch.Size([596571, 6])
Target shape: torch.Size([596571, 1])
Weights shape: torch.Size([596571, 1])
</pre></div>
</div>
</div>
</div>
<section id="data-splitting">
<h2>Data splitting<a class="headerlink" href="#data-splitting" title="Link to this heading">#</a></h2>
<p>It is very common in machine learning to split data into multiple independent sets, and only use part of the data for training/optimizing the machine learning models, and the rest for testing/evaluating performance. This is to check for possible over-fitting.</p>
<p>In the following, we will split the whole data into 50% training set, 25% validation set and 25% test set:</p>
<ul class="simple">
<li><p><strong>Training Dataset:</strong> The sample of data used to fit the model.</p></li>
<li><p><strong>Validation Dataset:</strong> The sample used to provide an unbiased evaluation of a model fit on the training dataset while tuning  hyperparameters.</p></li>
<li><p><strong>Test Dataset:</strong> The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset.</p></li>
</ul>
<p>We’ll do this using the very commonly used <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from Scikit Learn</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html</a></p>
<section id="split-the-data-into-training-set-and-rest-validation-test">
<h3>Split the Data into training set and rest (validation + test)<a class="headerlink" href="#split-the-data-into-training-set-and-rest-validation-test" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">X_val_and_test</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">y_val_and_test</span><span class="p">,</span>
    <span class="n">weights_train</span><span class="p">,</span>
    <span class="n">weights_val_and_test</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train Shape: &quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train Shape: &quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Weights shape: &quot;</span><span class="p">,</span> <span class="n">weights_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_val_and_test Shape: &quot;</span><span class="p">,</span> <span class="n">X_val_and_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_val_and_test Shape: &quot;</span><span class="p">,</span> <span class="n">y_val_and_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights_val_and_test shape: &quot;</span><span class="p">,</span> <span class="n">weights_val_and_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X_train Shape:  torch.Size([298285, 6])
y_train Shape:  torch.Size([298285, 1])
Training Weights shape:  torch.Size([298285, 1]) 

X_val_and_test Shape:  torch.Size([298286, 6])
y_val_and_test Shape:  torch.Size([298286, 1])
weights_val_and_test shape:  torch.Size([298286, 1])
</pre></div>
</div>
</div>
</div>
</section>
<section id="split-the-rest-val-test-into-validation-set-and-test-set">
<h3>Split the rest (val + test) into validation set and test set<a class="headerlink" href="#split-the-rest-val-test-into-validation-set-and-test-set" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">(</span>
    <span class="n">X_test</span><span class="p">,</span>
    <span class="n">X_val</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">y_val</span><span class="p">,</span>
    <span class="n">weights_test</span><span class="p">,</span>
    <span class="n">weights_val</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_val_and_test</span><span class="p">,</span> <span class="n">y_val_and_test</span><span class="p">,</span> <span class="n">weights_val_and_test</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_train Shape: &quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train Shape: &quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights_train shape: &quot;</span><span class="p">,</span> <span class="n">weights_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_val Shape: &quot;</span><span class="p">,</span> <span class="n">X_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_val Shape: &quot;</span><span class="p">,</span> <span class="n">y_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights_val shape: &quot;</span><span class="p">,</span> <span class="n">weights_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X_test Shape: &quot;</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_test Shape: &quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;weights_test shape: &quot;</span><span class="p">,</span> <span class="n">weights_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X_train Shape:  torch.Size([298285, 6])
y_train Shape:  torch.Size([298285, 1])
weights_train shape:  torch.Size([298285, 1]) 

X_val Shape:  torch.Size([149143, 6])
y_val Shape:  torch.Size([149143, 1])
weights_val shape:  torch.Size([149143, 1]) 

X_test Shape:  torch.Size([149143, 6])
y_test Shape:  torch.Size([149143, 1])
weights_test shape:  torch.Size([149143, 1])
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="building-the-neural-network">
<h2>Building the neural network<a class="headerlink" href="#building-the-neural-network" title="Link to this heading">#</a></h2>
<p>Now for the fun bit.
We’ll use Pytorch to build a simple multi-layer perceptron neural networks.
It will have two hidden layers, with 256 nodes each, interleaves with ReLU activation functions.
The last layer uses the Sigmoid activation to output a classifier score that ranges from 0 to 1.</p>
<p>In PyTorch, all neural networks are subclasses of the <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> class.
In the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function (constructor), we’ll initialize all of the network’s weights and biases.
Then we need to define a forward function, which implements the forward pass through the network.
Rememeber we also want to make the input pre-processing part of the network as well.
We can do all of this in just a few lines of code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">HiggsNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the network.</span>
<span class="sd">        Args:</span>
<span class="sd">            input_dim (int): The number of input features.</span>
<span class="sd">            means (torch.Tensor): The mean of the input features, shape (input_dim,).</span>
<span class="sd">            stds (torch.Tensor): The standard deviation of the input features, shape (input_dim,).</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>  <span class="c1"># This calls the parent class&#39;s constructor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>

        <span class="c1"># Here we register the means and stds as parameters, but we don&#39;t want to train them!</span>
        <span class="c1"># This is accomplished by setting requires_grad to False.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stds</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">stds</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear_relu_stack</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Forward pass through the network.</span>
<span class="sd">        Args:</span>
<span class="sd">            x (torch.Tensor): The input features, shape (batch_size, input_dim).</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.Tensor: The output of the network, shape (batch_size, 1).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Rescale the input features to have mean 0 and std 1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stds</span>

        <span class="c1"># Forward pass through the network</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_relu_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s initialize the network and print the architecture using the torchinfo library</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HiggsNet</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
<span class="n">torchinfo</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
HiggsNet                                 12
├─Sequential: 1-1                        --
│    └─Linear: 2-1                       1,792
│    └─ReLU: 2-2                         --
│    └─Linear: 2-3                       65,792
│    └─ReLU: 2-4                         --
│    └─Linear: 2-5                       257
│    └─Sigmoid: 2-6                      --
=================================================================
Total params: 67,853
Trainable params: 67,841
Non-trainable params: 12
=================================================================
</pre></div>
</div>
</div>
</div>
<p>Our network has 67,853 total parameters, the vast majority of which are contained in the weights which map between the two hidden layers.
Note there are also 12 non-trainable parameters, which are the 6 means and 6 standard deviations that will scale our inputs.
Let’s see if we can execute a forward pass:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_input</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test input shape: </span><span class="si">{</span><span class="n">test_input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">test_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output shape: </span><span class="si">{</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output: </span><span class="si">{</span><span class="n">test_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test input shape: torch.Size([10, 6])
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">RuntimeError</span><span class="g g-Whitespace">                              </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">25</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="n">test_input</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">10</span><span class="p">,:]</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test input shape: </span><span class="si">{</span><span class="n">test_input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">test_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output shape: </span><span class="si">{</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output: </span><span class="si">{</span><span class="n">test_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nn">File /global/common/software/m3246/conda/lbl-school/lib/python3.12/site-packages/torch/nn/modules/module.py:1751,</span> in <span class="ni">Module._wrapped_call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1749</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compiled_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore[misc]</span>
<span class="g g-Whitespace">   </span><span class="mi">1750</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1751</span>     <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_impl</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File /global/common/software/m3246/conda/lbl-school/lib/python3.12/site-packages/torch/nn/modules/module.py:1762,</span> in <span class="ni">Module._call_impl</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1757</span> <span class="c1"># If we don&#39;t have any hooks, we want to skip the rest of the logic in</span>
<span class="g g-Whitespace">   </span><span class="mi">1758</span> <span class="c1"># this function, and just call forward.</span>
<span class="g g-Whitespace">   </span><span class="mi">1759</span> <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_pre_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_hooks</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_pre_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1760</span>         <span class="ow">or</span> <span class="n">_global_backward_pre_hooks</span> <span class="ow">or</span> <span class="n">_global_backward_hooks</span>
<span class="g g-Whitespace">   </span><span class="mi">1761</span>         <span class="ow">or</span> <span class="n">_global_forward_hooks</span> <span class="ow">or</span> <span class="n">_global_forward_pre_hooks</span><span class="p">):</span>
<span class="ne">-&gt; </span><span class="mi">1762</span>     <span class="k">return</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1764</span> <span class="n">result</span> <span class="o">=</span> <span class="kc">None</span>
<span class="g g-Whitespace">   </span><span class="mi">1765</span> <span class="n">called_always_called_hooks</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

<span class="nn">Cell In[23], line 41,</span> in <span class="ni">HiggsNet.forward</span><span class="nt">(self, x)</span>
<span class="g g-Whitespace">     </span><span class="mi">31</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">32</span><span class="sd"> Forward pass through the network.</span>
<span class="g g-Whitespace">     </span><span class="mi">33</span><span class="sd"> Args:</span>
<span class="sd">   (...)     37     torch.Tensor: The output of the network, shape (batch_size, 1).</span>
<span class="g g-Whitespace">     </span><span class="mi">38</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">40</span> <span class="c1"># Rescale the input features to have mean 0 and std 1</span>
<span class="ne">---&gt; </span><span class="mi">41</span> <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stds</span>
<span class="g g-Whitespace">     </span><span class="mi">43</span> <span class="c1"># Forward pass through the network</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span> <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_relu_stack</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="ne">RuntimeError</span>: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
</pre></div>
</div>
</div>
</div>
<p>This should return an error, because by default PyTorch initializes the model weights and biases on the CPU, while we have all of our data on the GPU.
The solution is a simple “.to” call.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model parameters on device: </span><span class="si">{</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output shape: </span><span class="si">{</span><span class="n">test_output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test output: </span><span class="si">{</span><span class="n">test_output</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You should see that the network output on some random data points are around 0.5.
This is a good place to start for binary classification: the network is guessing equal probability of signal and background, having learned nothing yet.
If you don’t see this, it’s a good sign you haven’t scaled your inputs well, or something else is wrong.</p>
</section>
<section id="training-the-neural-network">
<h2>Training the neural network<a class="headerlink" href="#training-the-neural-network" title="Link to this heading">#</a></h2>
<p>Before we train, it’s a good idea to shuffle the training data.
The network would train fine without this step, but sometimes in HEP datasets the data can have hidden order you’re not aware of.
For example, there can be multiple MC samples pasted together, and if you don’t shuffle the network “sees” each one of these in sequence as it works through an epoch.
Such situations are best avoided.
We can shuffle the data using another utility function from sklearn</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">shuffle</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_train</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If we want to use a weighted loss, it&#39;s essential to set the reduction to &quot;none&quot;</span>
<span class="c1"># This way we can weight each event individually</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="c1"># We&#39;ll use the Adam optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="c1"># Some parameters for the training loop</span>
<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">train_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses_steps</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">val_losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># So we can align the validation loss with the training loss</span>

<span class="c1"># Epoch loop</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)):</span>

    <span class="c1"># Batch loop</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>

        <span class="c1"># Get a batch of data</span>
        <span class="n">X_batch</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">,:]</span>
        <span class="n">y_batch</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
        <span class="n">weights_batch</span> <span class="o">=</span> <span class="n">weights_train</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>

        <span class="c1"># Forward pass</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span>

        <span class="c1"># Weight the loss by the event weight, then take mean over the batch</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">weights_batch</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="c1"># Backward pass</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>  <span class="c1"># This important line clears the gradients from the previous batch</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Add the loss to the training loss list</span>
        <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="c1"># Validation step, we&#39;ll do this at the end of each epoch</span>
    <span class="c1"># Note I won&#39;t even both batching this step, since we have plenty of memory, but if you have a big dataset</span>
    <span class="c1"># you could do this in batches as well.</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>

        <span class="c1"># Weight the loss by the event weight, then take mean over the whole validation set</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">weights_val</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

        <span class="n">val_losses_steps</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">global_step</span><span class="p">)</span>
        <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="c1"># As a final step, shuffle the training data again to prepare for the next epoch</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_train</span> <span class="o">=</span> <span class="n">shuffle</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">weights_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot a loss curve</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">val_losses_steps</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Global Step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<section id="use-the-model-to-make-predicions">
<h3>Use the model to make predicions!<a class="headerlink" href="#use-the-model-to-make-predicions" title="Link to this heading">#</a></h3>
<p>Evaluate the model based on predictions made with X_test <span class="math notranslate nohighlight">\(\rightarrow\)</span> y_test</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run predictions on the training and testing set</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Flatten off the singleton dimension on the predictions, labels, and weights</span>
<span class="c1"># and convert to numpy arrays</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">y_pred_test</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y_pred_train</span> <span class="o">=</span> <span class="n">y_pred_train</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">weights_test</span> <span class="o">=</span> <span class="n">weights_test</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">weights_train</span> <span class="o">=</span> <span class="n">weights_train</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Print the first few predictions and labels</span>
<span class="k">with</span> <span class="n">np</span><span class="o">.</span><span class="n">printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_pred_test: &quot;</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_test: &quot;</span><span class="p">,</span> <span class="n">y_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_pred_train: &quot;</span><span class="p">,</span> <span class="n">y_pred_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y_train: &quot;</span><span class="p">,</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="roc-curves-and-area-under-the-curve-auc">
<h3>ROC curves and Area Under the Curve (AUC)<a class="headerlink" href="#roc-curves-and-area-under-the-curve-auc" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">roc_curve</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">roc_auc_score</span>

<span class="n">fpr_test</span><span class="p">,</span> <span class="n">tpr_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_pred_test</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights_test</span><span class="p">)</span>
<span class="n">fpr_train</span><span class="p">,</span> <span class="n">tpr_train</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_pred_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights_train</span><span class="p">)</span>
<span class="c1"># Note I&#39;m not using the weights here, &quot;roc_auc_score&quot; is an example of a function that can break with negative weights</span>
<span class="n">auc_test</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_pred_test</span><span class="p">)</span>
<span class="n">auc_train</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_pred_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_test</span><span class="p">,</span> <span class="n">tpr_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Test set auc: </span><span class="si">{</span><span class="n">auc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_train</span><span class="p">,</span> <span class="n">tpr_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Training set auc: </span><span class="si">{</span><span class="n">auc_train</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">extra_functions</span><span class="w"> </span><span class="kn">import</span> <span class="n">compare_train_test</span>

<span class="n">compare_train_test</span><span class="p">(</span>
    <span class="n">y_pred_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="n">y_pred_test</span><span class="p">,</span>
    <span class="n">y_test</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;NN Score&quot;</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;NN&quot;</span><span class="p">,</span>
    <span class="n">weights_train</span><span class="o">=</span><span class="n">weights_train</span><span class="p">,</span>
    <span class="n">weights_test</span><span class="o">=</span><span class="n">weights_test</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="significance-function">
<h2>Significance Function<a class="headerlink" href="#significance-function" title="Link to this heading">#</a></h2>
<p>In a standard statistical analysis for a search, an observed significance is obtained. This corresponds to the significance with which we reject the background-only hypothesis. That is, the higher the significance, the more likely the signal we are searching for exists.</p>
<p>We can use the formula below to obtain an estimated value of the expected significance.</p>
<p><span class="math notranslate nohighlight">\(Z = \sqrt{2+((s+b)\ln(1+s/b)-s)}\)</span></p>
<p><strong>see <a class="reference external" href="https://arxiv.org/pdf/1007.1727.pdf">arXiv:1007.1727</a> [Eq. 97]</strong></p>
<p>This corresponds to the signal sensitivity. This is a good indication of the performance of our classification model. =&gt; We want the significance to be as high as possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">sqrt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">log</span>


<span class="k">def</span><span class="w"> </span><span class="nf">amsasimov</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">s</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">((</span><span class="n">s</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">float</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">s</span><span class="p">))</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">float</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">((</span><span class="n">s</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nb">float</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">s</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>To get an idea of useful our classifier is, we can start by calculating the significance for an “inclusive analysis” that doesn’t cut on the classifier score in order to purify the signal region:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inclusive_significance</span> <span class="o">=</span> <span class="n">amsasimov</span><span class="p">(</span><span class="n">weights_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">weights_test</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Inclusive significance: </span><span class="si">{</span><span class="n">inclusive_significance</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The simplest way to use the classifier score to separate signal and background is to make a cut on the classifier score for all the events. In the following, we scan through the cut value and look at the corresponding significance. This can help us determine the optimal cut value of the classifier score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">int_pred_test_sig</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights_test</span><span class="p">[(</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred_test</span> <span class="o">&gt;</span> <span class="n">th_cut</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">th_cut</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)]</span>
<span class="n">int_pred_test_bkg</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights_test</span><span class="p">[(</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred_test</span> <span class="o">&gt;</span> <span class="n">th_cut</span><span class="p">)]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">for</span> <span class="n">th_cut</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">)]</span>
<span class="n">vamsasimov</span> <span class="o">=</span> <span class="p">[</span><span class="n">amsasimov</span><span class="p">(</span><span class="n">sumsig</span><span class="p">,</span> <span class="n">sumbkg</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">sumsig</span><span class="p">,</span> <span class="n">sumbkg</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">int_pred_test_sig</span><span class="p">,</span> <span class="n">int_pred_test_bkg</span><span class="p">)]</span>
<span class="n">Z</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">vamsasimov</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">),</span> <span class="n">vamsasimov</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Significance ($Z_</span><span class="se">{{</span><span class="s1">max</span><span class="se">}}</span><span class="s1"> = </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="w"> </span><span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">$)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hlines</span><span class="p">(</span><span class="n">inclusive_significance</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Inclusive significance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;NN Significance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Cut Value&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Significance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="avoiding-boilerplate-pytorch-lightning">
<h2>Avoiding boilerplate: Pytorch lightning<a class="headerlink" href="#avoiding-boilerplate-pytorch-lightning" title="Link to this heading">#</a></h2>
<p>If you kept using “pure pytorch” in your own work, after some time you’d realize that you spend a lot of time writing the same lines of code over and over again.
The perfect example of this is the training loop.
It will look very similar for training almost any neural network.
This kind of code is called “boilerplate”, and if you find youself writing a lot of it you’re probably wasting time in the writing and inevitable debugging.
This is why there are packages that aim to abstract away large chunks of the code needed to train a neural network.
Tensorflow, for example, ships with a package called Keras that does this.
The JAX tools are less developed, but are getting better and better.
For pytorch, there is a package called PyTorch lightning.</p>
<p>Let’s re-run training the network, but do it using lightning and see how much more efficient we can be.
Along the way, we’ll also use some data utility classes that ship with PyTorch, and automate a lot of the data handling steps we had to implement ourselves above</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lightning</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">L</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Central to pytorch lightning is the &quot;LightningModule&quot; class which organizes steps in the training loop into reusable pieces</span>
<span class="k">class</span><span class="w"> </span><span class="nc">HiggsNetLightning</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">HiggsNet</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;train_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Display the loss in the progress bar</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span> <span class="o">*</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Display the loss in the progress bar</span>
        <span class="k">return</span> <span class="n">loss</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_hat</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HiggsNetLightning</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">means</span><span class="o">=</span><span class="n">means</span><span class="p">,</span> <span class="n">stds</span><span class="o">=</span><span class="n">stds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we’re not going to write a training loop, what do we do with the data? Lightning expects the data to be wrapper in pytorch datasets and dataloaders, two classes that make training neural networks vastly easier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># A tensor dataset is a simple class that wraps a tensor and provides a way to iterate over it</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>

<span class="c1"># Datasets support splitting into training, validation, and test sets</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span><span class="p">,</span> <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">])</span>

<span class="c1"># Datasets can be used in a dataloader, which will automatically batch and shuffle the data</span>
<span class="c1"># We also don&#39;t need to use &quot;.to(device)&quot; here, as the dataloader will automatically move the data to the GPU</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Setting shuffle to True will shuffle the data every epoch</span>
<span class="n">val_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Some operations to get the data out of the dataset and dataloader classes</span>
<span class="n">first_five_events</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># This yields a tuple of tensors, in the order they were passed to the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">first_five_events</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target: </span><span class="si">{</span><span class="n">first_five_events</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights: </span><span class="si">{</span><span class="n">first_five_events</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># To get data out of the dataloader, you can iterate over it, or if you want one batch, build an iterable and use the &quot;next&quot; function</span>
<span class="n">first_batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">first_batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Target: </span><span class="si">{</span><span class="n">first_batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Weights: </span><span class="si">{</span><span class="n">first_batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The last thing we need is the &quot;Trainer&quot; class, which handles the training, validation, and testing of the model</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">L</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now we can run training with one line of code</span>
<span class="c1"># Note we also don&#39;t need to move the model to GPU, lightning does this for us</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># To run predictions, we can also use the trainer</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

<span class="c1"># Predictions are returned as a list of tensors, so we need to concatenate them</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">p</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">predictions</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Finally plot a ROC curve and calculate AUC</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[:][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">weights_test</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[:][</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="n">fpr_test</span><span class="p">,</span> <span class="n">tpr_test</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights_test</span><span class="p">)</span>
<span class="n">auc_test</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr_test</span><span class="p">,</span> <span class="n">tpr_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;tab:blue&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Test set auc: </span><span class="si">{</span><span class="n">auc_test</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver Operating Characteristic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>As a final note, for simple tasks like this one using all of these classes might seem a little silly, it’s not that much more work to just do it yourself.
However when you start to scale things to solve research problems, things can get complicated very quickly.
Some advanced topics that these packages will vastly simplify, at least in standard applications:</p>
<ul class="simple">
<li><p>Parallelized data loading: <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html">https://docs.pytorch.org/docs/stable/data.html</a></p></li>
<li><p>Experiment tracking: <a class="reference external" href="https://wandb.ai/site/">https://wandb.ai/site/</a> or <a class="reference external" href="https://www.comet.com/site/">https://www.comet.com/site/</a></p></li>
<li><p>Data parallel training: <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html">https://lightning.ai/docs/pytorch/stable/accelerators/gpu_intermediate.html</a></p></li>
<li><p>Model parallel training (god mode): <a class="reference external" href="https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html">https://lightning.ai/docs/pytorch/stable/advanced/model_parallel/fsdp.html</a></p></li>
</ul>
<p>In general, the more code you can offload onto someone else, the more time and frustration you’ll save yourself.
Packages are very helpful, use them as much as possible!</p>
<p>Some exercises to try if there’s time:</p>
<ol class="arabic simple">
<li><p>Try to get a higher AUC. Some ideas:</p></li>
</ol>
<ul class="simple">
<li><p>Adjust learning rate</p></li>
<li><p>Adjust the number of neurons, and layers</p></li>
<li><p>Increase epochs</p></li>
<li><p>Enable early stopping (and increase max epochs)</p></li>
<li><p>Adjust batch size</p></li>
<li><p>Change activations: [relu, leakyrelu, selu, tanh, gelu]</p></li>
<li><p>Try different optimizers (e.g. AdamW, Ranger, Lion)</p></li>
<li><p>Learning rate scheduler</p></li>
<li><p>Use a more complicated structure? This data is “tabular”, meaning we don’t have variable length inputs, so maybe don’t go there just yet</p></li>
<li><p>More… google!</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Break the model. Some ideas:</p></li>
</ol>
<ul class="simple">
<li><p>Get it to overfit</p></li>
<li><p>Don’t standardize the inputs</p></li>
<li><p>Use very unbalanced data</p></li>
<li><p>Mess with the weights</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Train a BDT instead</p></li>
</ol>
<ul class="simple">
<li><p>Lightning fast to train, and very very good at simple tasks</p></li>
<li><p>Would definitely cover this, if we had more time</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./sessions/day1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Binary classification on a HEP dataset</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#load-data">Load data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#event-selection">Event selection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data preprocessing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adjust-the-signal-background-weights-for-balanced-training">Adjust the Signal/Background Weights for balanced training</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-a-gpu">How to use a GPU</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-splitting">Data splitting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-the-data-into-training-set-and-rest-validation-test">Split the Data into training set and rest (validation + test)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#split-the-rest-val-test-into-validation-set-and-test-set">Split the rest (val + test) into validation set and test set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-the-neural-network">Building the neural network</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-neural-network">Training the neural network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#use-the-model-to-make-predicions">Use the model to make predicions!</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#roc-curves-and-area-under-the-curve-auc">ROC curves and Area Under the Curve (AUC)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-function">Significance Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#avoiding-boilerplate-pytorch-lightning">Avoiding boilerplate: Pytorch lightning</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Organizing team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright ML4FP 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>